"""
Backend FastAPI pour le projet MLOps Bank Churn
Charge automatiquement le meilleur mod√®le depuis les fichiers pickle et expose un endpoint de pr√©diction
"""

import os
import pickle
from typing import List, Dict, Any, Optional
import pandas as pd
import numpy as np
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
import uvicorn
from pathlib import Path

# ============================================================================
# CONFIGURATION
# ============================================================================

# Chemins vers les mod√®les et preprocessors
BASE_DIR = Path(__file__).parent.parent  # Racine du projet
MODELS_DIR = BASE_DIR / "notebooks"  # Dossier contenant les mod√®les .pkl
PROCESSORS_DIR = MODELS_DIR / "processors"  # Dossier contenant les preprocessors

# Configuration MLflow
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://127.0.0.1:5000")

# Liste des mod√®les disponibles avec leurs performances estim√©es
# (√Ä ajuster selon vos r√©sultats d'entra√Ænement)
AVAILABLE_MODELS = {
    "Stacking_LR_ensemble.pkl": {"name": "Stacking Ensemble", "priority": 1},
    "Voting_Soft_ensemble.pkl": {"name": "Voting Ensemble", "priority": 2},
    "XGBoost_tuned.pkl": {"name": "XGBoost Tuned", "priority": 3},
    "CatBoost_tuned.pkl": {"name": "CatBoost Tuned", "priority": 4},
    "LightGBM_tuned.pkl": {"name": "LightGBM Tuned", "priority": 5},
    "Random_Forest_tuned.pkl": {"name": "Random Forest Tuned", "priority": 6},
}

# ============================================================================
# MOD√àLES PYDANTIC POUR VALIDATION DES DONN√âES
# ============================================================================

class CustomerData(BaseModel):
    """
    Mod√®le de donn√©es pour une pr√©diction individuelle
    Accepte les features m√©tier simples et reconstruit automatiquement
    toutes les features avanc√©es c√¥t√© backend
    """
    # Features de base du client
    CreditScore: int = Field(..., description="Score de cr√©dit du client (300-850)")
    Geography: str = Field(..., description="Pays du client (France, Germany, Spain)")
    Gender: str = Field(..., description="Genre du client (Male, Female)")
    Age: int = Field(None, description="√Çge du client (optionnel si DateOfBirth fourni)")
    DateOfBirth: Optional[str] = Field(None, description="Date de naissance (YYYY-MM-DD)")
    
    # Informations bancaires
    Balance: float = Field(..., description="Solde du compte")
    NumOfProducts: int = Field(..., description="Nombre de produits (1-4)")
    HasCrCard: int = Field(..., description="Poss√®de une carte de cr√©dit (0 ou 1)")
    IsActiveMember: int = Field(..., description="Membre actif (0 ou 1)")
    CustomerTenure: int = Field(..., description="Anciennet√© en mois")
    
    # Informations financi√®res
    Income: float = Field(..., description="Revenu annuel")
    OutstandingLoans: float = Field(..., description="Pr√™ts en cours")
    EstimatedSalary: float = Field(..., description="Salaire estim√©")
    
    # Informations personnelles
    NumberOfDependents: int = Field(..., description="Nombre de personnes √† charge")
    Occupation: str = Field(..., description="Profession du client")
    MaritalStatus: str = Field(..., description="Statut marital")
    EducationLevel: str = Field(..., description="Niveau d'√©ducation")
    
    # Informations comportementales
    CustomerSegment: str = Field(..., description="Segment client")
    PreferredCommunicationChannel: str = Field(..., description="Canal de communication pr√©f√©r√©")
    NumComplaints: int = Field(0, description="Nombre de plaintes")
    
    class Config:
        json_schema_extra = {
            "example": {
                "CreditScore": 650,
                "Geography": "France",
                "Gender": "Female",
                "Age": 42,
                "Balance": 125000.0,
                "NumOfProducts": 2,
                "HasCrCard": 1,
                "IsActiveMember": 1,
                "CustomerTenure": 24,
                "Income": 85000.0,
                "OutstandingLoans": 15000.0,
                "EstimatedSalary": 85000.0,
                "NumberOfDependents": 2,
                "Occupation": "Engineer",
                "MaritalStatus": "Married",
                "EducationLevel": "Bachelor",
                "CustomerSegment": "Premium",
                "PreferredCommunicationChannel": "Email",
                "NumComplaints": 0
            }
        }

class PredictionResponse(BaseModel):
    """
    Mod√®le de r√©ponse pour une pr√©diction
    """
    prediction: int = Field(..., description="Pr√©diction (0 = reste, 1 = churn)")
    probability: float = Field(..., description="Probabilit√© de churn")
    model_version: str = Field(..., description="Version du mod√®le utilis√©")

# ============================================================================
# INITIALISATION DE L'APPLICATION FASTAPI
# ============================================================================

app = FastAPI(
    title="Bank Churn Prediction API",
    description="API de pr√©diction de churn bancaire utilisant MLflow",
    version="1.0.0"
)

# Variables globales pour stocker le mod√®le, les preprocessors et les informations
model = None
label_encoders = None
scaler = None
feature_names = None
model_info = {}

# ============================================================================
# FONCTIONS DE CHARGEMENT DU MOD√àLE ET PREPROCESSORS
# ============================================================================

def load_preprocessors():
    """
    Charge les preprocessors (label encoders, scaler, feature names) depuis le dossier processors
    """
    global label_encoders, scaler, feature_names
    
    try:
        # Charger les label encoders
        label_encoders_path = PROCESSORS_DIR / "label_encoders.pkl"
        if label_encoders_path.exists():
            with open(label_encoders_path, 'rb') as f:
                label_encoders = pickle.load(f)
            print(f"‚úÖ Label encoders charg√©s depuis {label_encoders_path}")
        else:
            print(f"‚ö†Ô∏è  Label encoders non trouv√©s: {label_encoders_path}")
        
        # Charger le scaler
        scaler_path = PROCESSORS_DIR / "scaler.pkl"
        if scaler_path.exists():
            with open(scaler_path, 'rb') as f:
                scaler = pickle.load(f)
            print(f"‚úÖ Scaler charg√© depuis {scaler_path}")
        else:
            print(f"‚ö†Ô∏è  Scaler non trouv√©: {scaler_path}")
        
        # Charger les noms de features
        feature_names_path = PROCESSORS_DIR / "feature_names.pkl"
        if feature_names_path.exists():
            with open(feature_names_path, 'rb') as f:
                feature_names = pickle.load(f)
            print(f"‚úÖ Feature names charg√©s depuis {feature_names_path}")
        else:
            print(f"‚ö†Ô∏è  Feature names non trouv√©s: {feature_names_path}")
            
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement des preprocessors: {str(e)}")
        raise

def load_best_model():
    """
    Charge le meilleur mod√®le disponible depuis les fichiers pickle
    Essaie de charger les mod√®les dans l'ordre de priorit√© d√©fini
    """
    global model, model_info
    
    try:
        print(f"üîç Recherche du meilleur mod√®le dans: {MODELS_DIR}")
        
        # Charger d'abord les preprocessors
        load_preprocessors()
        
        # Essayer de charger les mod√®les dans l'ordre de priorit√©
        models_by_priority = sorted(AVAILABLE_MODELS.items(), key=lambda x: x[1]["priority"])
        
        model_loaded = False
        for model_filename, model_metadata in models_by_priority:
            model_path = MODELS_DIR / model_filename
            
            if model_path.exists():
                try:
                    with open(model_path, 'rb') as f:
                        model = pickle.load(f)
                    
                    model_info = {
                        "model_name": model_metadata["name"],
                        "model_file": model_filename,
                        "model_path": str(model_path),
                        "priority": model_metadata["priority"]
                    }
                    
                    print(f"üèÜ Mod√®le charg√© avec succ√®s!")
                    print(f"üì¶ Nom: {model_info['model_name']}")
                    print(f"üìÅ Fichier: {model_info['model_file']}")
                    
                    model_loaded = True
                    break
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è  Erreur lors du chargement de {model_filename}: {str(e)}")
                    continue
        
        if not model_loaded:
            raise ValueError("Aucun mod√®le n'a pu √™tre charg√©. V√©rifiez que les fichiers .pkl existent dans le dossier notebooks/")
        
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du mod√®le: {str(e)}")
        raise

def engineer_features(data: pd.DataFrame) -> pd.DataFrame:
    """
    Applique le feature engineering complet pour recr√©er toutes les features
    utilis√©es lors de l'entra√Ænement du mod√®le
    
    Args:
        data: DataFrame avec les donn√©es brutes du client
    
    Returns:
        DataFrame avec toutes les features engineered
    """
    try:
        df = data.copy()
        
        # ============================================================================
        # 1. CALCUL DE L'√ÇGE SI N√âCESSAIRE
        # ============================================================================
        if 'DateOfBirth' in df.columns and df['DateOfBirth'].notna().any():
            reference_date = pd.Timestamp.now()
            df['DateOfBirth'] = pd.to_datetime(df['DateOfBirth'], errors='coerce')
            df['Age'] = (reference_date - df['DateOfBirth']).dt.days / 365.25
            df['Age'] = df['Age'].round(0).astype(int)
            df = df.drop(columns=['DateOfBirth'])
        
        # ============================================================================
        # 2. FEATURE ENGINEERING - Cr√©er les features d√©riv√©es
        # ============================================================================
        
        # Income per dependent
        df['Income_Per_Dependent'] = df['Income'] / (df['NumberOfDependents'] + 1)
        
        # Balance per product
        df['Balance_Per_Product'] = df['Balance'] / df['NumOfProducts']
        
        # Credit utilization
        df['Credit_Utilization'] = df['OutstandingLoans'] / df['Income']
        
        # Loan to balance ratio
        df['Loan_To_Balance_Ratio'] = df['OutstandingLoans'] / (df['Balance'] + 1)
        
        # Tenure groups
        df['Tenure_Group'] = pd.cut(df['CustomerTenure'],
                                     bins=[0, 6, 12, 24, 30],
                                     labels=['0-6m', '6-12m', '1-2y', '2y+'])
        
        # Credit score categories
        df['Credit_Category'] = pd.cut(df['CreditScore'],
                                        bins=[0, 579, 669, 739, 799, 850],
                                        labels=['Poor', 'Fair', 'Good', 'Very Good', 'Excellent'])
        
        # Products per year (engagement metric)
        df['Products_Per_Year'] = df['NumOfProducts'] / (df['CustomerTenure'] / 12 + 0.1)
        
        # Complaints per year
        df['Complaints_Per_Year'] = df['NumComplaints'] / (df['CustomerTenure'] / 12 + 0.1)
        
        # Age groups
        df['Age_Group'] = pd.cut(df['Age'],
                                 bins=[0, 25, 35, 45, 55, 65, 100],
                                 labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])
        
        # High value customer flag (utilise des quantiles fixes bas√©s sur l'entra√Ænement)
        # Note: Id√©alement, ces seuils devraient √™tre sauvegard√©s lors de l'entra√Ænement
        balance_threshold = 100000  # Approximation du 75e percentile
        df['High_Value_Customer'] = ((df['Balance'] > balance_threshold) & 
                                      (df['NumOfProducts'] >= 3)).astype(int)
        
        # At-risk flag (utilise des m√©dianes fixes bas√©es sur l'entra√Ænement)
        complaints_median = 1  # Approximation
        balance_median = 50000  # Approximation
        df['At_Risk'] = ((df['NumComplaints'] > complaints_median) & 
                         (df['Balance'] < balance_median)).astype(int)
        
        print(f"‚úÖ Feature engineering termin√©: {df.shape[1]} colonnes cr√©√©es")
        return df
        
    except Exception as e:
        print(f"‚ùå Erreur lors du feature engineering: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

def encode_features(data: pd.DataFrame) -> pd.DataFrame:
    """
    Applique l'encodage des variables cat√©gorielles
    
    Args:
        data: DataFrame avec les features engineered
    
    Returns:
        DataFrame avec les features encod√©es
    """
    try:
        df = data.copy()
        
        # ============================================================================
        # 1. BINARY ENCODING - Gender
        # ============================================================================
        df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})
        
        # ============================================================================
        # 2. ONE-HOT ENCODING - Variables cat√©gorielles
        # ============================================================================
        categorical_to_encode = [
            'MaritalStatus', 'EducationLevel', 'CustomerSegment',
            'PreferredCommunicationChannel', 'Age_Group', 'Tenure_Group',
            'Credit_Category'
        ]
        
        df = pd.get_dummies(df, columns=categorical_to_encode, drop_first=True, dtype=int)
        
        # ============================================================================
        # 3. LABEL ENCODING - Occupation (haute cardinalit√©)
        # ============================================================================
        if 'Occupation' in df.columns and label_encoders is not None:
            if 'Occupation' in label_encoders:
                encoder = label_encoders['Occupation']
                try:
                    df['Occupation_Encoded'] = encoder.transform(df['Occupation'])
                except ValueError:
                    # Si une valeur inconnue, utiliser la premi√®re classe
                    print(f"‚ö†Ô∏è  Occupation inconnue, utilisation de la valeur par d√©faut")
                    df['Occupation_Encoded'] = encoder.transform([encoder.classes_[0]])[0]
                df = df.drop(columns=['Occupation'])
        
        print(f"‚úÖ Encodage termin√©: {df.shape[1]} colonnes")
        return df
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'encodage: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

def preprocess_input(data: pd.DataFrame) -> pd.DataFrame:
    """
    Pipeline complet de preprocessing:
    1. Feature engineering
    2. Encodage des variables cat√©gorielles
    3. Scaling
    
    Args:
        data: DataFrame avec les donn√©es brutes
    
    Returns:
        DataFrame avec les donn√©es preprocess√©es pr√™tes pour la pr√©diction
    """
    try:
        # √âtape 1: Feature engineering
        df = engineer_features(data)
        
        # √âtape 2: Encodage
        df = encode_features(df)
        
        # √âtape 3: Scaling
        if scaler is not None:
            # R√©cup√©rer l'ordre des colonnes depuis feature_names
            if feature_names is not None and isinstance(feature_names, dict):
                numerical_cols = feature_names.get('numerical_features', [])
                all_features_expected = feature_names.get('all_features', [])
                
                # V√©rifier quelles colonnes sont pr√©sentes
                missing_cols = [col for col in all_features_expected if col not in df.columns]
                if missing_cols:
                    print(f"‚ö†Ô∏è  Colonnes manquantes: {missing_cols[:5]}...")  # Afficher les 5 premi√®res
                    # Ajouter les colonnes manquantes avec des z√©ros
                    for col in missing_cols:
                        df[col] = 0
                
                # R√©organiser les colonnes dans le bon ordre
                df = df[all_features_expected]
                
                # Appliquer le scaling uniquement sur les colonnes num√©riques
                df_scaled = df.copy()
                if numerical_cols:
                    df_scaled[numerical_cols] = scaler.transform(df[numerical_cols])
                    df = df_scaled
            else:
                # Fallback: scaler toutes les colonnes
                df_scaled = scaler.transform(df)
                df = pd.DataFrame(df_scaled, columns=df.columns)
        
        print(f"‚úÖ Preprocessing complet: {df.shape}")
        return df
        
    except Exception as e:
        print(f"‚ùå Erreur lors du preprocessing: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

# ============================================================================
# √âV√âNEMENTS DE D√âMARRAGE/ARR√äT
# ============================================================================

@app.on_event("startup")
async def startup_event():
    """
    √âv√©nement ex√©cut√© au d√©marrage de l'application
    Charge le meilleur mod√®le depuis MLflow
    """
    print("üöÄ D√©marrage de l'API Bank Churn Prediction")
    load_best_model()
    print("‚úÖ API pr√™te √† recevoir des requ√™tes")

@app.on_event("shutdown")
async def shutdown_event():
    """
    √âv√©nement ex√©cut√© √† l'arr√™t de l'application
    """
    print("üõë Arr√™t de l'API Bank Churn Prediction")

# ============================================================================
# ENDPOINTS
# ============================================================================

@app.get("/")
async def root():
    """
    Endpoint racine - Informations sur l'API
    """
    return {
        "message": "Bank Churn Prediction API",
        "version": "1.0.0",
        "status": "running",
        "model_loaded": model is not None,
        "endpoints": {
            "health": "/health",
            "predict": "/predict (POST)",
            "model_info": "/model-info"
        }
    }

@app.get("/health")
async def health_check():
    """
    Endpoint de v√©rification de sant√©
    V√©rifie que le mod√®le est charg√© et pr√™t
    """
    if model is None:
        raise HTTPException(status_code=503, detail="Mod√®le non charg√©")
    
    return {
        "status": "healthy",
        "model_loaded": True,
        "mlflow_uri": MLFLOW_TRACKING_URI
    }

@app.get("/model-info")
async def get_model_info():
    """
    Endpoint pour obtenir les informations sur le mod√®le charg√©
    """
    if model is None:
        raise HTTPException(status_code=503, detail="Mod√®le non charg√©")
    
    return {
        "model_info": model_info,
        "preprocessors_loaded": {
            "label_encoders": label_encoders is not None,
            "scaler": scaler is not None,
            "feature_names": feature_names is not None
        }
    }

@app.post("/predict", response_model=PredictionResponse)
async def predict(customer: CustomerData):
    """
    Endpoint de pr√©diction
    Accepte les donn√©es d'un client et retourne la pr√©diction de churn
    
    Args:
        customer: Donn√©es du client (CustomerData)
    
    Returns:
        PredictionResponse: Pr√©diction (0/1) et probabilit√© de churn
    """
    if model is None:
        raise HTTPException(
            status_code=503, 
            detail="Mod√®le non disponible. Veuillez v√©rifier que les mod√®les sont charg√©s."
        )
    
    try:
        # Convertir les donn√©es Pydantic en DataFrame
        customer_dict = customer.dict()
        df = pd.DataFrame([customer_dict])
        
        print(f"üì• Requ√™te de pr√©diction re√ßue: {customer_dict}")
        
        # Appliquer le preprocessing
        df_preprocessed = preprocess_input(df)
        
        print(f"üîÑ Donn√©es preprocess√©es: {df_preprocessed.values[0][:5]}...")  # Afficher les 5 premi√®res valeurs
        
        # Faire la pr√©diction
        prediction = model.predict(df_preprocessed)
        
        # Obtenir les probabilit√©s si disponibles
        try:
            probabilities = model.predict_proba(df_preprocessed)
            # Probabilit√© de la classe 1 (churn)
            churn_probability = float(probabilities[0][1])
        except AttributeError:
            # Si predict_proba n'est pas disponible, utiliser la pr√©diction binaire
            churn_probability = float(prediction[0])
        
        # Convertir la pr√©diction en entier (0 ou 1)
        prediction_value = int(prediction[0])
        
        print(f"‚úÖ Pr√©diction: {prediction_value}, Probabilit√©: {churn_probability:.4f}")
        
        return PredictionResponse(
            prediction=prediction_value,
            probability=round(churn_probability, 4),
            model_version=model_info.get("model_file", "unknown")
        )
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la pr√©diction: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de la pr√©diction: {str(e)}"
        )

# ============================================================================
# POINT D'ENTR√âE
# ============================================================================

if __name__ == "__main__":
    # Lancer le serveur uvicorn
    uvicorn.run(
        "api:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )
