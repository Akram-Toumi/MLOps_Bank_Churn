"""
Script de comparaison de mod√®les pour MLOps
√Ä int√©grer dans churn_predection.ipynb √† la fin du notebook

Ce script:
1. Charge le meilleur mod√®le des 12 entra√Æn√©s
2. Charge le mod√®le actuellement en production (depuis MLflow ou fichier)
3. Compare leurs performances
4. D√©cide si on d√©ploie le nouveau mod√®le
"""

import pickle
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report
import mlflow
import mlflow.sklearn
from pathlib import Path
import json

# ============================================================================
# CONFIGURATION
# ============================================================================

PRODUCTION_MODEL_PATH = "models/production_model.pkl"  # Mod√®le actuellement en prod
PRODUCTION_METRICS_PATH = "models/production_metrics.json"  # M√©triques du mod√®le prod
IMPROVEMENT_THRESHOLD = 0.02  # 2% d'am√©lioration minimum pour d√©ployer

# ============================================================================
# FONCTION: √âvaluer un mod√®le
# ============================================================================

def evaluate_model(model, X_test, y_test):
    """√âvalue un mod√®le et retourne ses m√©triques"""
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred
    
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'f1_score': f1_score(y_test, y_pred),
        'roc_auc': roc_auc_score(y_test, y_pred_proba)
    }
    
    return metrics

# ============================================================================
# FONCTION: Comparer deux mod√®les
# ============================================================================

def compare_models(new_metrics, prod_metrics, threshold=0.02):
    """
    Compare les m√©triques de deux mod√®les
    
    Returns:
        dict: {
            'deploy': bool,
            'reason': str,
            'improvements': dict
        }
    """
    improvements = {
        'accuracy': new_metrics['accuracy'] - prod_metrics['accuracy'],
        'f1_score': new_metrics['f1_score'] - prod_metrics['f1_score'],
        'roc_auc': new_metrics['roc_auc'] - prod_metrics['roc_auc']
    }
    
    # D√©cision bas√©e sur F1-score (m√©trique principale)
    f1_improvement = improvements['f1_score']
    
    if f1_improvement > threshold:
        decision = {
            'deploy': True,
            'reason': f'Am√©lioration significative du F1-score: +{f1_improvement:.2%}',
            'improvements': improvements
        }
    elif f1_improvement > 0:
        decision = {
            'deploy': False,
            'reason': f'Am√©lioration trop faible ({f1_improvement:.2%} < {threshold:.2%})',
            'improvements': improvements
        }
    else:
        decision = {
            'deploy': False,
            'reason': f'D√©gradation des performances: {f1_improvement:.2%}',
            'improvements': improvements
        }
    
    return decision

# ============================================================================
# FONCTION: Sauvegarder le mod√®le en production
# ============================================================================

def save_production_model(model, metrics, model_name):
    """Sauvegarde le mod√®le comme nouveau mod√®le de production"""
    # Cr√©er le dossier models s'il n'existe pas
    Path("models").mkdir(exist_ok=True)
    
    # Sauvegarder le mod√®le
    with open(PRODUCTION_MODEL_PATH, 'wb') as f:
        pickle.dump(model, f)
    
    # Sauvegarder les m√©triques
    metrics_to_save = {
        **metrics,
        'model_name': model_name,
        'deployment_date': pd.Timestamp.now().isoformat()
    }
    
    with open(PRODUCTION_METRICS_PATH, 'w') as f:
        json.dump(metrics_to_save, f, indent=2)
    
    print(f"‚úÖ Mod√®le {model_name} sauvegard√© en production")
    print(f"   Accuracy: {metrics['accuracy']:.4f}")
    print(f"   F1-Score: {metrics['f1_score']:.4f}")
    print(f"   ROC-AUC: {metrics['roc_auc']:.4f}")

# ============================================================================
# FONCTION: Logger dans MLflow
# ============================================================================

def log_to_mlflow(model, metrics, model_name, tags=None):
    """Log le mod√®le et ses m√©triques dans MLflow"""
    with mlflow.start_run(run_name=model_name):
        # Log des m√©triques
        mlflow.log_metrics(metrics)
        
        # Log du mod√®le
        mlflow.sklearn.log_model(model, "model")
        
        # Log des tags
        if tags:
            mlflow.set_tags(tags)
        
        print(f"‚úÖ Mod√®le {model_name} logg√© dans MLflow")

# ============================================================================
# SCRIPT PRINCIPAL √Ä INT√âGRER DANS LE NOTEBOOK
# ============================================================================

"""
# ============================================================================
# √âTAPE FINALE: COMPARAISON AVEC LE MOD√àLE EN PRODUCTION
# ============================================================================

print("="*80)
print("COMPARAISON AVEC LE MOD√àLE EN PRODUCTION")
print("="*80)

# 1. Identifier le meilleur mod√®le des 12 entra√Æn√©s
# (Supposons que vous avez d√©j√† un dictionnaire 'all_models_results' avec les r√©sultats)

best_model_name = max(all_models_results, key=lambda x: all_models_results[x]['f1_score'])
best_model = trained_models[best_model_name]  # Votre mod√®le entra√Æn√©
best_metrics = all_models_results[best_model_name]

print(f"\\nüèÜ Meilleur mod√®le entra√Æn√©: {best_model_name}")
print(f"   F1-Score: {best_metrics['f1_score']:.4f}")

# 2. Charger le mod√®le actuellement en production
production_exists = Path(PRODUCTION_MODEL_PATH).exists()

if production_exists:
    print(f"\\nüì¶ Chargement du mod√®le en production...")
    
    with open(PRODUCTION_MODEL_PATH, 'rb') as f:
        production_model = pickle.load(f)
    
    with open(PRODUCTION_METRICS_PATH, 'r') as f:
        production_metrics = json.load(f)
    
    print(f"   Mod√®le actuel: {production_metrics.get('model_name', 'Unknown')}")
    print(f"   F1-Score: {production_metrics['f1_score']:.4f}")
    
    # 3. Comparer les mod√®les
    print(f"\\nüîç Comparaison des performances...")
    decision = compare_models(best_metrics, production_metrics, IMPROVEMENT_THRESHOLD)
    
    print(f"\\n{'='*80}")
    print("D√âCISION DE D√âPLOIEMENT")
    print(f"{'='*80}")
    print(f"\\nD√©ployer le nouveau mod√®le: {'‚úÖ OUI' if decision['deploy'] else '‚ùå NON'}")
    print(f"Raison: {decision['reason']}")
    print(f"\\nAm√©liorations:")
    for metric, improvement in decision['improvements'].items():
        sign = '+' if improvement > 0 else ''
        print(f"  ‚Ä¢ {metric}: {sign}{improvement:.2%}")
    
    # 4. D√©ployer si d√©cision positive
    if decision['deploy']:
        print(f"\\nüöÄ D√©ploiement du nouveau mod√®le...")
        save_production_model(best_model, best_metrics, best_model_name)
        log_to_mlflow(best_model, best_metrics, best_model_name, 
                     tags={'status': 'production', 'replaced': production_metrics.get('model_name')})
    else:
        print(f"\\n‚è∏Ô∏è  Conservation du mod√®le actuel en production")
        print(f"   Le nouveau mod√®le n'apporte pas d'am√©lioration suffisante")

else:
    # Pas de mod√®le en production, d√©ployer directement
    print(f"\\n‚ö†Ô∏è  Aucun mod√®le en production d√©tect√©")
    print(f"   D√©ploiement automatique du meilleur mod√®le...")
    save_production_model(best_model, best_metrics, best_model_name)
    log_to_mlflow(best_model, best_metrics, best_model_name, 
                 tags={'status': 'production', 'first_deployment': True})

print(f"\\n{'='*80}")
print("‚úÖ PROCESSUS TERMIN√â")
print(f"{'='*80}")
"""
