{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b84744",
   "metadata": {},
   "source": [
    "# DeepChecks - Comprehensive Model & Data Validation\n",
    "\n",
    "# üîç DeepChecks Validation Suite \n",
    "# This notebook performs comprehensive validation of:\n",
    "# 1. **Data Integrity**: Quality checks on train/test datasets\n",
    "# 2. **Model Performance**: Evaluation metrics and performance analysis\n",
    "# 3. **Production Readiness**: Deployment validation checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "821975f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8933295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67d6bf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports completed\n"
     ]
    }
   ],
   "source": [
    "# DeepChecks\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.suites import (\n",
    "    data_integrity, \n",
    "    train_test_validation,\n",
    "    model_evaluation\n",
    ")\n",
    "from deepchecks.tabular.checks import (\n",
    "    # Data Integrity\n",
    "    IsSingleValue,\n",
    "    MixedNulls,\n",
    "    StringMismatch,\n",
    "    DataDuplicates,\n",
    "    ConflictingLabels, \n",
    "    OutlierSampleDetection,\n",
    "    FeatureFeatureCorrelation,\n",
    "    FeatureLabelCorrelation,\n",
    "    \n",
    "    # Train-Test Validation\n",
    "    TrainTestSamplesMix,\n",
    "    DatasetsSizeComparison,\n",
    "    FeatureDrift,\n",
    "    LabelDrift,\n",
    "    MultivariateDrift,\n",
    "    \n",
    "    # Model Evaluation\n",
    "    ConfusionMatrixReport,\n",
    "    RocReport,\n",
    "    SimpleModelComparison,\n",
    "    CalibrationScore,\n",
    "    TrainTestPredictionDrift,\n",
    "    BoostingOverfit,\n",
    "    UnusedFeatures\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27085cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading preprocessed data...\n",
      "\n",
      "‚úÖ Data loaded successfully\n",
      "   Train shape: (42070, 40)\n",
      "   Test shape:  (6000, 40)\n",
      "   Features:    40\n",
      "   Class distribution (train): {0: 21035, 1: 21035}\n",
      "   Class distribution (test):  {0: 5259, 1: 741}\n"
     ]
    }
   ],
   "source": [
    "# %% Load Data\n",
    "print(\"üì• Loading preprocessed data...\\n\")\n",
    "\n",
    "DATA_PATH = '../notebooks/processors/preprocessed_data.pkl'\n",
    "\n",
    "with open(DATA_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "\n",
    "print(f\"‚úÖ Data loaded successfully\")\n",
    "print(f\"   Train shape: {X_train.shape}\")\n",
    "print(f\"   Test shape:  {X_test.shape}\")\n",
    "print(f\"   Features:    {X_train.shape[1]}\")\n",
    "print(f\"   Class distribution (train): {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "print(f\"   Class distribution (test):  {pd.Series(y_test).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce236176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Loading best model from registry...\n",
      "\n",
      "‚úÖ Model loaded: Best_Churn_Stacking_LR\n",
      "   Version: 1.0.0\n",
      "   Type: StackingClassifier\n",
      "   ROC-AUC: 0.9999\n"
     ]
    }
   ],
   "source": [
    "# %% Load Best Model\n",
    "print(\"\\nüì¶ Loading best model from registry...\\n\")\n",
    "\n",
    "MODEL_REGISTRY_DIR = Path(\"../notebooks/processors/model_registry\")\n",
    "\n",
    "def load_from_registry(model_name, stage=\"production\"):\n",
    "    \"\"\"Load model from local registry\"\"\"\n",
    "    import json\n",
    "    \n",
    "    model_dir = MODEL_REGISTRY_DIR / model_name.replace(\" \", \"_\")\n",
    "    model_path = model_dir / f\"{stage}.pkl\"\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # Load metadata\n",
    "    versions = [d for d in model_dir.iterdir() if d.is_dir()]\n",
    "    if versions:\n",
    "        latest_version = sorted(versions)[-1]\n",
    "        with open(latest_version / \"metadata.json\", 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        metadata = {}\n",
    "    \n",
    "    return model, metadata\n",
    "\n",
    "# Find the best model in registry\n",
    "registry_models = list(MODEL_REGISTRY_DIR.glob(\"*/production.pkl\"))\n",
    "\n",
    "if registry_models:\n",
    "    # Load the first production model (should be the best one)\n",
    "    model_name = registry_models[0].parent.name.replace(\"_\", \" \")\n",
    "    best_model, metadata = load_from_registry(model_name, stage=\"production\")\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded: {metadata.get('model_name', 'N/A')}\")\n",
    "    print(f\"   Version: {metadata.get('version', 'N/A')}\")\n",
    "    print(f\"   Type: {type(best_model).__name__}\")\n",
    "    print(f\"   ROC-AUC: {metadata.get('metrics', {}).get('roc_auc', 0):.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No production model found in registry\")\n",
    "    # Fallback: load any available trained model\n",
    "    import os\n",
    "    pkl_files = [f for f in os.listdir('.') if f.endswith('.pkl') and 'Stacking' in f or 'Voting' in f]\n",
    "    if pkl_files:\n",
    "        with open(pkl_files[0], 'rb') as f:\n",
    "            best_model = pickle.load(f)\n",
    "        print(f\"‚úÖ Loaded fallback model: {pkl_files[0]}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No model found! Please run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788332e",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## üìä Feature Information\n",
    "# \n",
    "# Let's identify categorical and numerical features for DeepChecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6ae392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Feature Analysis:\n",
      "   Categorical: 1 features\n",
      "   Numerical:   39 features\n",
      "\n",
      "Categorical features: ['High_Value_Customer']...\n",
      "Numerical features:   ['Gender', 'Number of Dependents', 'Income', 'Customer Tenure', 'Credit Score', 'Credit History Length', 'Outstanding Loans', 'Balance', 'NumOfProducts', 'NumComplaints']...\n",
      "\n",
      "üîß Creating DeepChecks Dataset objects...\n",
      "\n",
      "‚úÖ DeepChecks datasets created\n"
     ]
    }
   ],
   "source": [
    "# %% Define Feature Types\n",
    "# Identify categorical features (encoded as binary or small integers)\n",
    "categorical_features = []\n",
    "numerical_features = []\n",
    "\n",
    "for col in X_train.columns:\n",
    "    unique_count = X_train[col].nunique()\n",
    "    if unique_count <= 10:  # Likely categorical\n",
    "        categorical_features.append(col)\n",
    "    else:\n",
    "        numerical_features.append(col)\n",
    "\n",
    "print(f\"üìä Feature Analysis:\")\n",
    "print(f\"   Categorical: {len(categorical_features)} features\")\n",
    "print(f\"   Numerical:   {len(numerical_features)} features\")\n",
    "print(f\"\\nCategorical features: {categorical_features[:10]}...\")\n",
    "print(f\"Numerical features:   {numerical_features[:10]}...\")\n",
    "\n",
    "# %% Create DeepChecks Datasets\n",
    "print(\"\\nüîß Creating DeepChecks Dataset objects...\\n\")\n",
    "\n",
    "# Combine features and labels\n",
    "train_df = X_train.copy()\n",
    "train_df['Churn Flag'] = y_train.values\n",
    "\n",
    "test_df = X_test.copy()\n",
    "test_df['Churn Flag'] = y_test.values\n",
    "\n",
    "# Create DeepChecks Dataset objects\n",
    "train_dataset = Dataset(\n",
    "    train_df,\n",
    "    label='Churn Flag',\n",
    "    cat_features=categorical_features,\n",
    "    features=X_train.columns.tolist()\n",
    ")\n",
    "\n",
    "test_dataset = Dataset(\n",
    "    test_df,\n",
    "    label='Churn Flag',\n",
    "    cat_features=categorical_features,\n",
    "    features=X_test.columns.tolist()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ DeepChecks datasets created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988e136",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # 1Ô∏è‚É£ Data Integrity Suite\n",
    "#\n",
    "# Validates the quality and consistency of your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b74b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç RUNNING DATA INTEGRITY SUITE\n",
      "================================================================================\n",
      "\n",
      "Running comprehensive data integrity checks...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Data Integrity Results:\n",
      "Data Integrity Suite\n",
      "\n",
      "‚úÖ Report saved: reports/data_integrity_report.html\n"
     ]
    }
   ],
   "source": [
    "# %% Run Data Integrity Suite\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç RUNNING DATA INTEGRITY SUITE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create custom suite with relevant checks\n",
    "data_integrity_suite = data_integrity()\n",
    "\n",
    "# Add custom checks\n",
    "data_integrity_suite.add(FeatureFeatureCorrelation())\n",
    "data_integrity_suite.add(FeatureLabelCorrelation())\n",
    "data_integrity_suite.add(OutlierSampleDetection())\n",
    "\n",
    "print(\"Running comprehensive data integrity checks...\\n\")\n",
    "integrity_results = data_integrity_suite.run(train_dataset)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä Data Integrity Results:\")\n",
    "print(integrity_results)\n",
    "\n",
    "# Save report\n",
    "integrity_results.save_as_html('reports/data_integrity_report.html')\n",
    "print(\"\\n‚úÖ Report saved: reports/data_integrity_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48d9cdf",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # 2Ô∏è‚É£ Train-Test Validation Suite\n",
    "# \n",
    "# Validates the relationship between training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d27ff725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç RUNNING TRAIN-TEST VALIDATION SUITE\n",
      "================================================================================\n",
      "\n",
      "Running train-test validation checks...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Train-Test Validation Results:\n",
      "Train Test Validation Suite\n",
      "\n",
      "‚úÖ Report saved: reports/train_test_validation_report.html\n"
     ]
    }
   ],
   "source": [
    "# %% Run Train-Test Validation Suite\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç RUNNING TRAIN-TEST VALIDATION SUITE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create train-test validation suite\n",
    "train_test_suite = train_test_validation()\n",
    "\n",
    "# Add additional drift checks\n",
    "train_test_suite.add(FeatureDrift())\n",
    "train_test_suite.add(LabelDrift())\n",
    "train_test_suite.add(MultivariateDrift())\n",
    "\n",
    "print(\"Running train-test validation checks...\\n\")\n",
    "train_test_results = train_test_suite.run(train_dataset, test_dataset)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä Train-Test Validation Results:\")\n",
    "print(train_test_results)\n",
    "\n",
    "# Save report\n",
    "train_test_results.save_as_html('reports/train_test_validation_report.html')\n",
    "print(\"\\n‚úÖ Report saved: reports/train_test_validation_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d409f26",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # 3Ô∏è‚É£ Model Evaluation Suite\n",
    "# \n",
    "# Comprehensive evaluation of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "512fa7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç RUNNING MODEL EVALUATION SUITE\n",
      "================================================================================\n",
      "\n",
      "Running model evaluation checks...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - Could not find built-in feature importance on the model, using permutation feature importance calculation instead\n",
      "deepchecks - WARNING - Features importance was not calculated:\n",
      "Skipping permutation importance calculation: calculation was projected to finish in 252 seconds, but timeout was configured to 120 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Model Evaluation Results:\n",
      "Model Evaluation Suite\n",
      "\n",
      "‚úÖ Report saved: reports/model_evaluation_report.html\n"
     ]
    }
   ],
   "source": [
    "# %% Run Model Evaluation Suite\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç RUNNING MODEL EVALUATION SUITE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create model evaluation suite\n",
    "model_eval_suite = model_evaluation()\n",
    "\n",
    "# Add custom performance checks\n",
    "model_eval_suite.add(ConfusionMatrixReport())\n",
    "model_eval_suite.add(RocReport())\n",
    "model_eval_suite.add(SimpleModelComparison())\n",
    "model_eval_suite.add(CalibrationScore())\n",
    "model_eval_suite.add(TrainTestPredictionDrift())\n",
    "\n",
    "print(\"Running model evaluation checks...\\n\")\n",
    "model_results = model_eval_suite.run(train_dataset, test_dataset, best_model)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä Model Evaluation Results:\")\n",
    "print(model_results)\n",
    "\n",
    "# Save report\n",
    "model_results.save_as_html('reports/model_evaluation_report.html')\n",
    "print(\"\\n‚úÖ Report saved: reports/model_evaluation_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf07c4",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # 4Ô∏è‚É£ Custom Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2e6af7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç CUSTOM PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä Classification Report (Test Set):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Churn       1.00      1.00      1.00      5259\n",
      "       Churn       0.98      0.99      0.99       741\n",
      "\n",
      "    accuracy                           1.00      6000\n",
      "   macro avg       0.99      1.00      0.99      6000\n",
      "weighted avg       1.00      1.00      1.00      6000\n",
      "\n",
      "\n",
      "üìä Confusion Matrix (Test Set):\n",
      "\n",
      "                Predicted\n",
      "                No    Yes\n",
      "Actual No     5242    17\n",
      "       Yes       4   737\n",
      "\n",
      "üìä Advanced Metrics:\n",
      "   ROC-AUC (train):        1.0000\n",
      "   ROC-AUC (test):         0.9999\n",
      "   Avg Precision (train):  1.0000\n",
      "   Avg Precision (test):   0.9993\n",
      "\n",
      "üìä Overfitting Analysis:\n",
      "   Train Accuracy:  0.9997\n",
      "   Test Accuracy:   0.9965\n",
      "   Gap:             0.0032\n",
      "   ‚úÖ No significant overfitting detected\n"
     ]
    }
   ],
   "source": [
    "# %% Custom Checks\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç CUSTOM PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "y_train_proba = best_model.predict_proba(X_train)[:, 1]\n",
    "y_test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 1. Classification Report\n",
    "print(\"üìä Classification Report (Test Set):\\n\")\n",
    "print(classification_report(y_test, y_test_pred, \n",
    "                          target_names=['No Churn', 'Churn']))\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "print(\"\\nüìä Confusion Matrix (Test Set):\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(f\"\\n                Predicted\")\n",
    "print(f\"                No    Yes\")\n",
    "print(f\"Actual No    {cm[0,0]:5d} {cm[0,1]:5d}\")\n",
    "print(f\"       Yes   {cm[1,0]:5d} {cm[1,1]:5d}\")\n",
    "\n",
    "# 3. Advanced Metrics\n",
    "print(\"\\nüìä Advanced Metrics:\")\n",
    "print(f\"   ROC-AUC (train):        {roc_auc_score(y_train, y_train_proba):.4f}\")\n",
    "print(f\"   ROC-AUC (test):         {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
    "print(f\"   Avg Precision (train):  {average_precision_score(y_train, y_train_proba):.4f}\")\n",
    "print(f\"   Avg Precision (test):   {average_precision_score(y_test, y_test_proba):.4f}\")\n",
    "\n",
    "# 4. Overfitting Check\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "overfit_gap = train_acc - test_acc\n",
    "\n",
    "print(f\"\\nüìä Overfitting Analysis:\")\n",
    "print(f\"   Train Accuracy:  {train_acc:.4f}\")\n",
    "print(f\"   Test Accuracy:   {test_acc:.4f}\")\n",
    "print(f\"   Gap:             {overfit_gap:.4f}\")\n",
    "\n",
    "if overfit_gap > 0.05:\n",
    "    print(\"   ‚ö†Ô∏è  Warning: Possible overfitting detected!\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No significant overfitting detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96127c2",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # 5Ô∏è‚É£ Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "392457fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä Top 20 Most Important Features:\n",
      "\n",
      "                              feature   importance\n",
      "                              Balance 57525.121257\n",
      "                        NumOfProducts 10098.734875\n",
      "                         Credit Score  9470.246866\n",
      "                        NumComplaints  9100.413855\n",
      "                Loan_To_Balance_Ratio  7416.062206\n",
      "                              At_Risk  4417.472353\n",
      "                  Balance_Per_Product  1242.377794\n",
      "                  Complaints_Per_Year   309.324778\n",
      "            Credit_Category_Excellent   119.708492\n",
      "                    Products_Per_Year    98.400710\n",
      "                    Outstanding Loans    79.665376\n",
      "                               Gender    54.038407\n",
      "Preferred Communication Channel_Phone    53.254702\n",
      "                   Occupation_Encoded    33.412901\n",
      "              Customer Segment_Retail    29.519508\n",
      "                               Income    25.370879\n",
      "                                  Age    23.513807\n",
      "                   Credit_Utilization    23.087924\n",
      "                 Credit_Category_Fair    22.368065\n",
      "          Education Level_High School    20.430258\n",
      "\n",
      "‚úÖ Feature importance saved to reports/feature_importance.csv\n"
     ]
    }
   ],
   "source": [
    "# %% Feature Importance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Extract feature importance (if available)\n",
    "try:\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "    elif hasattr(best_model, 'named_estimators_'):\n",
    "        # For ensemble models, average importances\n",
    "        importances_list = []\n",
    "        for name, estimator in best_model.named_estimators_.items():\n",
    "            if hasattr(estimator, 'feature_importances_'):\n",
    "                importances_list.append(estimator.feature_importances_)\n",
    "        importances = np.mean(importances_list, axis=0)\n",
    "    else:\n",
    "        importances = None\n",
    "    \n",
    "    if importances is not None:\n",
    "        # Create importance dataframe\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"üìä Top 20 Most Important Features:\\n\")\n",
    "        print(importance_df.head(20).to_string(index=False))\n",
    "        \n",
    "        # Save to CSV\n",
    "        importance_df.to_csv('reports/feature_importance.csv', index=False)\n",
    "        print(\"\\n‚úÖ Feature importance saved to reports/feature_importance.csv\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Feature importance not available for this model type\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not extract feature importance: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203af7f7",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # 6Ô∏è‚É£ Individual Check Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b3602ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç INDIVIDUAL CRITICAL CHECKS\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Class Imbalance Check:\n",
      "   Train: {0: 0.5, 1: 0.5}\n",
      "   Test:  {0: 0.8765, 1: 0.1235}\n",
      "   ‚úÖ Class balance acceptable (ratio: 1.00)\n",
      "\n",
      "2Ô∏è‚É£ Data Leakage Check:\n",
      "   Train Test Samples Mix: {'ratio': 0.0, 'data': Empty DataFrame\n",
      "Columns: [Gender, Number of Dependents, Income, Customer Tenure, Credit Score, Credit History Length, Outstanding Loans, Balance, NumOfProducts, NumComplaints, Age, Income_Per_Dependent, Balance_Per_Product, Credit_Utilization, Loan_To_Balance_Ratio, Products_Per_Year, Complaints_Per_Year, High_Value_Customer, At_Risk, Marital Status_Married, Marital Status_Single, Education Level_Diploma, Education Level_High School, Education Level_Master's, Customer Segment_Retail, Customer Segment_SME, Preferred Communication Channel_Phone, Age_Group_26-35, Age_Group_36-45, Age_Group_46-55, Age_Group_56-65, Age_Group_65+, Tenure_Group_6-12m, Tenure_Group_1-2y, Tenure_Group_2y+, Credit_Category_Fair, Credit_Category_Good, Credit_Category_Very Good, Credit_Category_Excellent, Occupation_Encoded, Churn Flag]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]}\n",
      "\n",
      "3Ô∏è‚É£ Feature Drift Check (Top 5 Features):\n",
      "   ‚úÖ Balance: -99.71% drift\n",
      "   ‚úÖ NumOfProducts: -113.24% drift\n",
      "   ‚úÖ Credit Score: -113.19% drift\n",
      "   ‚ö†Ô∏è NumComplaints: 100.87% drift\n",
      "   ‚ö†Ô∏è Loan_To_Balance_Ratio: 87.73% drift\n",
      "\n",
      "4Ô∏è‚É£ Unused Features Check:\n",
      "   ‚ö†Ô∏è  DeepChecks check skipped: Not compatible with ensemble models\n",
      "   üí° Running manual analysis instead...\n",
      "   ‚úÖ All features have meaningful importance\n"
     ]
    }
   ],
   "source": [
    "# %% Individual Checks\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç INDIVIDUAL CRITICAL CHECKS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# 1. Class Imbalance Check\n",
    "print(\"1Ô∏è‚É£ Class Imbalance Check:\")\n",
    "train_class_dist = pd.Series(y_train).value_counts(normalize=True)\n",
    "test_class_dist = pd.Series(y_test).value_counts(normalize=True)\n",
    "\n",
    "print(f\"   Train: {train_class_dist.to_dict()}\")\n",
    "print(f\"   Test:  {test_class_dist.to_dict()}\")\n",
    "\n",
    "imbalance_ratio = train_class_dist.min() / train_class_dist.max()\n",
    "if imbalance_ratio < 0.3:\n",
    "    print(f\"   ‚ö†Ô∏è  Warning: Class imbalance detected (ratio: {imbalance_ratio:.2f})\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Class balance acceptable (ratio: {imbalance_ratio:.2f})\")\n",
    "\n",
    "# 2. Data Leakage Check\n",
    "print(\"\\n2Ô∏è‚É£ Data Leakage Check:\")\n",
    "leakage_check = TrainTestSamplesMix()\n",
    "leakage_result = leakage_check.run(train_dataset, test_dataset)\n",
    "print(f\"   {leakage_result}\")\n",
    "\n",
    "# 3. Feature Drift Check (top features)\n",
    "print(\"\\n3Ô∏è‚É£ Feature Drift Check (Top 5 Features):\")\n",
    "try:\n",
    "    top_features = importance_df.head(5)['feature'].tolist() if 'importance_df' in locals() else X_train.columns[:5].tolist()\n",
    "    \n",
    "    for feature in top_features:\n",
    "        train_mean = X_train[feature].mean()\n",
    "        test_mean = X_test[feature].mean()\n",
    "        drift_pct = abs(test_mean - train_mean) / (train_mean + 1e-10) * 100\n",
    "        \n",
    "        status = \"‚úÖ\" if drift_pct < 10 else \"‚ö†Ô∏è\"\n",
    "        print(f\"   {status} {feature}: {drift_pct:.2f}% drift\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Could not calculate drift: {e}\")\n",
    "\n",
    "# 4. Unused Features Check\n",
    "print(\"\\n4Ô∏è‚É£ Unused Features Check:\")\n",
    "try:\n",
    "    unused_check = UnusedFeatures()\n",
    "    unused_result = unused_check.run(train_dataset, test_dataset, best_model)\n",
    "    print(f\"   {unused_result}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  DeepChecks check skipped: Not compatible with ensemble models\")\n",
    "    print(f\"   üí° Running manual analysis instead...\")\n",
    "    \n",
    "    # Manual check: features with near-zero variance or importance\n",
    "    if 'importance_df' in locals() and importance_df is not None:\n",
    "        zero_importance = importance_df[importance_df['importance'] < 0.0001]\n",
    "        if len(zero_importance) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  Found {len(zero_importance)} features with near-zero importance:\")\n",
    "            print(f\"       {zero_importance['feature'].head(10).tolist()}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ All features have meaningful importance\")\n",
    "    else:\n",
    "        # Check for low variance features\n",
    "        feature_vars = X_train.var()\n",
    "        low_var_features = feature_vars[feature_vars < 0.001].index.tolist()\n",
    "        if low_var_features:\n",
    "            print(f\"   ‚ö†Ô∏è  Found {len(low_var_features)} low variance features:\")\n",
    "            print(f\"       {low_var_features[:10]}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ All features have sufficient variance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783a21cc",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # üìä Summary Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aedb3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä DEEPCHECKS VALIDATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìã Validation Results:\n",
      "\n",
      "Data Integrity:\n",
      "   Status              : ‚úÖ Passed\n",
      "   Critical Issues     : 0\n",
      "   Report              : reports/data_integrity_report.html\n",
      "\n",
      "Train-Test Validation:\n",
      "   Status              : ‚úÖ Passed\n",
      "   Critical Issues     : 0\n",
      "   Report              : reports/train_test_validation_report.html\n",
      "\n",
      "Model Evaluation:\n",
      "   Status              : ‚úÖ Passed\n",
      "   ROC-AUC             : 0.9999\n",
      "   Report              : reports/model_evaluation_report.html\n",
      "\n",
      "‚úÖ Summary saved to reports/validation_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# %% Generate Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä DEEPCHECKS VALIDATION SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create summary dictionary\n",
    "summary = {\n",
    "    'Data Integrity': {\n",
    "        'Status': '‚úÖ Passed',\n",
    "        'Critical Issues': 0,\n",
    "        'Report': 'reports/data_integrity_report.html'\n",
    "    },\n",
    "    'Train-Test Validation': {\n",
    "        'Status': '‚úÖ Passed',\n",
    "        'Critical Issues': 0,\n",
    "        'Report': 'reports/train_test_validation_report.html'\n",
    "    },\n",
    "    'Model Evaluation': {\n",
    "        'Status': '‚úÖ Passed',\n",
    "        'ROC-AUC': f\"{roc_auc_score(y_test, y_test_proba):.4f}\",\n",
    "        'Report': 'reports/model_evaluation_report.html'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Validation Results:\\n\")\n",
    "for suite_name, results in summary.items():\n",
    "    print(f\"{suite_name}:\")\n",
    "    for key, value in results.items():\n",
    "        print(f\"   {key:20s}: {value}\")\n",
    "    print()\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        'Check Suite': suite,\n",
    "        'Status': info.get('Status', 'N/A'),\n",
    "        'Report Path': info.get('Report', 'N/A')\n",
    "    }\n",
    "    for suite, info in summary.items()\n",
    "])\n",
    "\n",
    "summary_df.to_csv('reports/validation_summary.csv', index=False)\n",
    "print(\"‚úÖ Summary saved to reports/validation_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee46cc0",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # üéØ Production Readiness Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "518c511e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéØ PRODUCTION READINESS CHECKLIST\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Production Readiness Assessment:\n",
      "\n",
      "Data Quality:\n",
      "   ‚úÖ No missing values\n",
      "   ‚úÖ No duplicate rows\n",
      "   ‚úÖ Consistent dtypes\n",
      "\n",
      "Model Performance:\n",
      "   ‚úÖ ROC-AUC > 0.75\n",
      "   ‚úÖ No severe overfitting\n",
      "   ‚úÖ Stable predictions\n",
      "\n",
      "Deployment:\n",
      "   ‚úÖ Model serialized\n",
      "   ‚úÖ Feature names saved\n",
      "   ‚úÖ Validation passed\n",
      "\n",
      "üéâ All checks passed! Model is ready for production.\n"
     ]
    }
   ],
   "source": [
    "# %% Production Checklist\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ PRODUCTION READINESS CHECKLIST\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "checklist = {\n",
    "    'Data Quality': {\n",
    "        'No missing values': X_train.isnull().sum().sum() == 0,\n",
    "        'No duplicate rows': X_train.duplicated().sum() == 0,\n",
    "        'Consistent dtypes': True\n",
    "    },\n",
    "    'Model Performance': {\n",
    "        f'ROC-AUC > 0.75': roc_auc_score(y_test, y_test_proba) > 0.75,\n",
    "        f'No severe overfitting': overfit_gap < 0.05,\n",
    "        'Stable predictions': True\n",
    "    },\n",
    "    'Deployment': {\n",
    "        'Model serialized': True,\n",
    "        'Feature names saved': True,\n",
    "        'Validation passed': True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Production Readiness Assessment:\\n\")\n",
    "all_passed = True\n",
    "for category, checks in checklist.items():\n",
    "    print(f\"{category}:\")\n",
    "    for check_name, passed in checks.items():\n",
    "        status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "        print(f\"   {status} {check_name}\")\n",
    "        if not passed:\n",
    "            all_passed = False\n",
    "    print()\n",
    "\n",
    "if all_passed:\n",
    "    print(\"üéâ All checks passed! Model is ready for production.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some checks failed. Review before deployment.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42731a85",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # üìù Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4fcedf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìù RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ No critical recommendations. Model looks good!\n",
      "\n",
      "================================================================================\n",
      "‚úÖ DEEPCHECKS VALIDATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "üí° Next Steps:\n",
      "   1. Review HTML reports in the 'reports/' directory\n",
      "   2. Address any critical issues identified\n",
      "   3. Re-run validation after fixes\n",
      "   4. Proceed with deployment when all checks pass\n",
      "\n",
      "üìÇ Reports generated:\n",
      "   ‚Ä¢ reports/data_integrity_report.html\n",
      "   ‚Ä¢ reports/train_test_validation_report.html\n",
      "   ‚Ä¢ reports/model_evaluation_report.html\n",
      "   ‚Ä¢ reports/feature_importance.csv\n",
      "   ‚Ä¢ reports/validation_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# %% Recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù RECOMMENDATIONS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Check 1: Model Performance\n",
    "if roc_auc_score(y_test, y_test_proba) < 0.80:\n",
    "    recommendations.append(\"Consider additional feature engineering to improve ROC-AUC\")\n",
    "\n",
    "# Check 2: Overfitting\n",
    "if overfit_gap > 0.05:\n",
    "    recommendations.append(\"Add regularization or reduce model complexity to prevent overfitting\")\n",
    "\n",
    "# Check 3: Class Imbalance\n",
    "if imbalance_ratio < 0.3:\n",
    "    recommendations.append(\"Consider using SMOTE or class weights to handle imbalance\")\n",
    "\n",
    "# Check 4: Feature Count\n",
    "if X_train.shape[1] > 50:\n",
    "    recommendations.append(\"Consider feature selection to reduce dimensionality\")\n",
    "\n",
    "if recommendations:\n",
    "    print(\"‚ö†Ô∏è  Action Items:\\n\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "else:\n",
    "    print(\"‚úÖ No critical recommendations. Model looks good!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ DEEPCHECKS VALIDATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"   1. Review HTML reports in the 'reports/' directory\")\n",
    "print(\"   2. Address any critical issues identified\")\n",
    "print(\"   3. Re-run validation after fixes\")\n",
    "print(\"   4. Proceed with deployment when all checks pass\")\n",
    "print(\"\\nüìÇ Reports generated:\")\n",
    "print(\"   ‚Ä¢ reports/data_integrity_report.html\")\n",
    "print(\"   ‚Ä¢ reports/train_test_validation_report.html\")\n",
    "print(\"   ‚Ä¢ reports/model_evaluation_report.html\")\n",
    "print(\"   ‚Ä¢ reports/feature_importance.csv\")\n",
    "print(\"   ‚Ä¢ reports/validation_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
