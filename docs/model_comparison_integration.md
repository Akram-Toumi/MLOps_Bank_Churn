# Guide d'int√©gration - Comparaison de mod√®les

## Objectif
Modifier `churn_predection.ipynb` pour comparer le meilleur des 12 mod√®les avec le mod√®le en production avant d√©ploiement.

## √âtapes d'int√©gration

### 1. Ajouter les imports au d√©but du notebook

```python
import pickle
import json
from pathlib import Path
```

### 2. Copier les fonctions depuis `scripts/model_comparison.py`

Copier ces fonctions dans une nouvelle cellule apr√®s l'entra√Ænement des 12 mod√®les :
- `evaluate_model()`
- `compare_models()`
- `save_production_model()`
- `log_to_mlflow()` (optionnel si MLflow d√©j√† configur√©)

### 3. Remplacer la cellule finale de d√©ploiement

**AVANT** (logique actuelle - √† supprimer) :
```python
# Sauvegarder le meilleur mod√®le
best_model_name = max(all_models_results, key=lambda x: all_models_results[x]['f1_score'])
best_model = trained_models[best_model_name]

with open(f'models/{best_model_name}_production.pkl', 'wb') as f:
    pickle.dump(best_model, f)
```

**APR√àS** (nouvelle logique - √† ajouter) :
```python
# ============================================================================
# COMPARAISON AVEC LE MOD√àLE EN PRODUCTION
# ============================================================================

PRODUCTION_MODEL_PATH = "models/production_model.pkl"
PRODUCTION_METRICS_PATH = "models/production_metrics.json"
IMPROVEMENT_THRESHOLD = 0.02  # 2%

print("="*80)
print("COMPARAISON AVEC LE MOD√àLE EN PRODUCTION")
print("="*80)

# 1. Meilleur mod√®le des 12 entra√Æn√©s
best_model_name = max(all_models_results, key=lambda x: all_models_results[x]['f1_score'])
best_model = trained_models[best_model_name]
best_metrics = all_models_results[best_model_name]

print(f"\\nüèÜ Meilleur mod√®le: {best_model_name}")
print(f"   F1-Score: {best_metrics['f1_score']:.4f}")

# 2. Charger mod√®le production si existe
if Path(PRODUCTION_MODEL_PATH).exists():
    with open(PRODUCTION_MODEL_PATH, 'rb') as f:
        production_model = pickle.load(f)
    with open(PRODUCTION_METRICS_PATH, 'r') as f:
        production_metrics = json.load(f)
    
    print(f"\\nüì¶ Mod√®le actuel: {production_metrics.get('model_name')}")
    print(f"   F1-Score: {production_metrics['f1_score']:.4f}")
    
    # 3. Comparer
    decision = compare_models(best_metrics, production_metrics, IMPROVEMENT_THRESHOLD)
    
    print(f"\\n{'='*80}")
    print(f"D√©ployer: {'‚úÖ OUI' if decision['deploy'] else '‚ùå NON'}")
    print(f"Raison: {decision['reason']}")
    
    # 4. D√©ployer si am√©lioration
    if decision['deploy']:
        save_production_model(best_model, best_metrics, best_model_name)
        print("üöÄ Nouveau mod√®le d√©ploy√©")
    else:
        print("‚è∏Ô∏è  Mod√®le actuel conserv√©")
else:
    # Premier d√©ploiement
    print("\\n‚ö†Ô∏è  Premier d√©ploiement")
    save_production_model(best_model, best_metrics, best_model_name)
```

### 4. Cr√©er le dossier models

```python
Path("models").mkdir(exist_ok=True)
```

## Variables n√©cessaires

Assurez-vous que votre notebook a ces variables :
- `all_models_results` : dict avec les m√©triques de tous les mod√®les
- `trained_models` : dict avec les mod√®les entra√Æn√©s
- `X_test`, `y_test` : donn√©es de test

## Exemple de structure `all_models_results`

```python
all_models_results = {
    'XGBoost_tuned': {
        'accuracy': 0.85,
        'f1_score': 0.82,
        'roc_auc': 0.88
    },
    'Random_Forest_tuned': {
        'accuracy': 0.83,
        'f1_score': 0.80,
        'roc_auc': 0.86
    },
    # ... autres mod√®les
}
```

## Test

Apr√®s int√©gration, ex√©cuter le notebook devrait :
1. ‚úÖ Entra√Æner les 12 mod√®les
2. ‚úÖ Identifier le meilleur
3. ‚úÖ Comparer avec le mod√®le prod (si existe)
4. ‚úÖ D√©ployer seulement si am√©lioration > 2%
5. ‚úÖ Sauvegarder dans `models/production_model.pkl`

## Fichiers cr√©√©s

- `models/production_model.pkl` : Mod√®le en production
- `models/production_metrics.json` : M√©triques du mod√®le prod
